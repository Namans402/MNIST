{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install numpy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.23.2'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import sklearn\n",
    "#sklearn.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WONT WORK WITH OLDER VERSIONS OF SCIKIT-LEARN\n",
    "# from sklearn.datasets import fetch_mldata\n",
    "# mnist = fetch_mldata('MNIST original')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True) # get MNIST dataset, save features to X and labels to y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X # X dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we want to make the pixel intensity range [0,1]\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X # X dataset after normalized "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resize y from 70000, 0 to 10, 70000\n",
    "# This way instead of the label being a number (ie, 5), we have a 2D array where the rowss are possible labels (ie 1 - 10)\n",
    "# and the columns are the images, we are essentially encoding the labels\n",
    "\n",
    "digits = 10\n",
    "examples = y.shape[0]\n",
    "\n",
    "y = y.reshape(1, examples)\n",
    "\n",
    "Y_new = np.eye(digits)[y.astype('int32')] # this is just preprocessing, try splitting each line independently \n",
    "Y_new = Y_new.T.reshape(digits, examples) # preparing canvas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]),\n",
       " (10, 70000))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_new, Y_new.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split X into train and test\n",
    "\n",
    "m = 60000\n",
    "m_test = X.shape[0] - m                                                 # m_test will have 10K features and train will have 60K\n",
    "\n",
    "X_train, X_test = X[:m].T, X[m:].T                                      # split the dataset into train and split\n",
    "Y_train, Y_test = Y_new[:,:m], Y_new[:,m:]\n",
    "\n",
    "shuffle_index = np.random.permutation(m)                                # randomly shuffle order\n",
    "X_train, Y_train = X_train[:, shuffle_index], Y_train[:, shuffle_index] # randomly shuffle data order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAEUklEQVR4nO3dMUqcWxiA4TtBsckWRBsbC0PKdJbBUusgWGcLWmQXqdNYDmQNbsBKUrkEE1Akzdz6gnO8Mmbm1TxP6cc/HJTXDzyMM5nNZv8APW9WfQDgYeKEKHFClDghSpwQtfbI3J9y4c+bPPRFmxOixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcELW26gPwvH7//j2cb2xsDOeHh4dzZ+fn58Nn19fXh3OexuaEKHFClDghSpwQJU6IEidEiROi3HO+Ml++fBnOJ5PJcD6dTufOvn79Onz28+fPwzlPY3NClDghSpwQJU6IEidEiROiJrPZbDQfDlm+m5ub4Xxzc3M4v729Hc5HVy07OzvDZ6+uroZz5nrwm25zQpQ4IUqcECVOiBInRIkTosQJUd4y9sJ8+/ZtOL+7u1vo9U9OTubOjo6OFnptnsbmhChxQpQ4IUqcECVOiBInRIkTotxz8h/v3r2bO/v48eMST4LNCVHihChxQpQ4IUqcECVOiBInRLnn/MusrY1/5FtbW0s6CY+xOSFKnBAlTogSJ0SJE6LECVHihCifz/nCvH//fji/vLwczt++fTuc//z588lnYmE+nxNeEnFClDghSpwQJU6IEidEectYzI8fP4bz6+vrhV5/b29voedZHpsTosQJUeKEKHFClDghSpwQJU6Ics8Zc3FxMZz/+vVrodc/ODhY6HmWx+aEKHFClDghSpwQJU6IEidEiROi3HP+ZT58+LDqI/A/2ZwQJU6IEidEiROixAlR4oQocUKUe86YRz6S8dH5Y/b39xd6nuWxOSFKnBAlTogSJ0SJE6LECVHihCj3nDGTyWShOa+HzQlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiPKWsZjpdLrqIxBhc0KUOCFKnBAlTogSJ0SJE6LECVHuOWOurq5WfQQibE6IEidEiROixAlR4oQocUKUOCHKPecr8+nTp1UfgWdic0KUOCFKnBAlTogSJ0SJE6JcpbwyZ2dnqz4Cz8TmhChxQpQ4IUqcECVOiBInRIkTotxzvjDHx8fD+fb29nIOwh9nc0KUOCFKnBAlTogSJ0SJE6LECVHuOVfg+/fvc2fX19fDZw8ODobzN2/8vn0t/CQhSpwQJU6IEidEiROixAlR4oQo95wrcH9/P3e2u7s7fPb09PS5j0OUzQlR4oQocUKUOCFKnBAlTogSJ0RNZrPZaD4cAs9i8tAXbU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFC1GMfAfjgv+wD/jybE6LECVHihChxQpQ4IUqcEPUvHm9cjPjbuDgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show a feature along it's label\n",
    "\n",
    "i = 12                                                                 # show feature number 12\n",
    "plt.imshow(X_train[:,i].reshape(28,28), cmap = matplotlib.cm.binary)   # display what it looks like\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "Y_train[:,i]                                                           # display label number 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_multiclass_loss(Y, Y_hat):\n",
    "    \n",
    "    # MSE\n",
    "    L_sum = np.sum((Y - Y_hat)**2)\n",
    "    m = Y.shape[1]\n",
    "    L = (1/m)*L_sum\n",
    "    \n",
    "    return L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try the efficient loss function as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_multiclass_loss_efficient(Y, Y_hat):\n",
    "\n",
    "    # Log Loss\n",
    "    L_sum = (np.sum(np.multiply(np.log(Y_hat),Y)) + np.sum(np.multiply(np.log(1-Y_hat),(1-Y))))\n",
    "    m = Y.shape[1]\n",
    "    L = -(1./m) * L_sum\n",
    "\n",
    "    return L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):               # sigmoid activation function\n",
    "    s = 1 / (1 + np.exp(-z))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 cost:  24.572985937963715\n",
      "Epoch 100 cost:  1.3489497382761166\n",
      "Epoch 200 cost:  1.0604464571037209\n",
      "Epoch 300 cost:  0.9247392491154036\n",
      "Epoch 400 cost:  0.8391376733855804\n",
      "Epoch 500 cost:  0.7776893123667894\n",
      "Epoch 600 cost:  0.7299604559331057\n",
      "Epoch 700 cost:  0.691186168555328\n",
      "Epoch 800 cost:  0.6587810948501509\n",
      "Epoch 900 cost:  0.6311639182821049\n",
      "Epoch 1000 cost:  0.6072515247224884\n",
      "Epoch 1100 cost:  0.5862527459338012\n",
      "Epoch 1200 cost:  0.567563384991765\n",
      "Epoch 1300 cost:  0.5507302351468718\n",
      "Epoch 1400 cost:  0.5354156959839514\n",
      "Epoch 1500 cost:  0.5213712413674982\n",
      "Epoch 1600 cost:  0.5084192925828382\n",
      "Epoch 1700 cost:  0.49642810257305664\n",
      "Epoch 1800 cost:  0.48528972319354485\n",
      "Epoch 1900 cost:  0.4749096614843133\n",
      "Final cost: 0.46529652958685946\n"
     ]
    }
   ],
   "source": [
    "n_x = X_train.shape[0]                               # number of inputs\n",
    "n_h = 64                                             # 1 hidden layer, 64 neurons\n",
    "learning_rate = 1\n",
    "\n",
    "W1 = np.random.randn(n_h, n_x)                       # assigne weights and biases with random values\n",
    "B1 = np.zeros((n_h, 1))                              # so h1 has 64 neurons\n",
    "W2 = np.random.randn(digits, n_h) \n",
    "B2 = np.zeros((digits, 1))                           # and output layer will have 10 neurons\n",
    "\n",
    "X = X_train                       \n",
    "Y = Y_train\n",
    "\n",
    "for i in range(2000):                                 # 2000 epochs\n",
    "\n",
    "    Z1 = np.matmul(W1, X) + B1                        # output, wx + b\n",
    "    A1 = sigmoid(Z1)                                  # activation output \n",
    "    Z2 = np.matmul(W2, A1) + B2\n",
    "    A2 = sigmoid(Z2)\n",
    "\n",
    "    cost = compute_multiclass_loss_efficient(Y, A2)   # calculate the loss\n",
    "\n",
    "    dZ2 = A2-Y                                        # calculate all the partial derivatives\n",
    "    dW2 = (1./m) * np.matmul(dZ2, A1.T)\n",
    "    dB2 = (1./m) * np.sum(dZ2, axis=1, keepdims=True) # keepdims formats the array\n",
    "\n",
    "    dA1 = np.matmul(W2.T, dZ2)\n",
    "    dZ1 = dA1 * sigmoid(Z1) * (1 - sigmoid(Z1))\n",
    "    dW1 = (1./m) * np.matmul(dZ1, X.T)\n",
    "    dB1 = (1./m) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "\n",
    "    W2 = W2 - learning_rate * dW2                     # update the weigths and biases\n",
    "    B2 = B2 - learning_rate * dB2\n",
    "    W1 = W1 - learning_rate * dW1\n",
    "    B1 = B1 - learning_rate * dB1\n",
    "\n",
    "    if (i % 100 == 0):                                # for every 100 epoch, we output the loss\n",
    "        print(\"Epoch\", i, \"cost: \", cost)\n",
    "\n",
    "print(\"Final cost:\", cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 957    0   12    6    2    9   16    2    5    8]\n",
      " [   0 1112    2    1    2    2    3   14    4    6]\n",
      " [   2    3  938   16    4    9    7   28    5    0]\n",
      " [   3    3   19  925    1   30    2    8   27    9]\n",
      " [   2    0   12    1  922    6   10    6   13   37]\n",
      " [   5    2    4   22    1  778    8    2   24   10]\n",
      " [   7    5   10    3    6   15  906    0    8    1]\n",
      " [   1    1   14    9    2    9    1  946    3   15]\n",
      " [   2    9   20   19    4   27    5    2  877   12]\n",
      " [   1    0    1    8   38    7    0   20    8  911]]\n"
     ]
    }
   ],
   "source": [
    "Z1 = np.matmul(W1, X_test) + B1                # calculate our network using our updated optimised weights and biases\n",
    "A1 = sigmoid(Z1)\n",
    "Z2 = np.matmul(W2, A1) + B2\n",
    "A2 = np.exp(Z2) / np.sum(np.exp(Z2), axis=0)\n",
    "\n",
    "predictions = np.argmax(A2, axis=0)            # get our predictions\n",
    "labels = np.argmax(Y_test, axis=0)             # get our labels\n",
    "\n",
    "print(confusion_matrix(predictions, labels))   # use confusion matrix to look at model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.94      0.96      1017\n",
      "           1       0.98      0.97      0.98      1146\n",
      "           2       0.91      0.93      0.92      1012\n",
      "           3       0.92      0.90      0.91      1027\n",
      "           4       0.94      0.91      0.93      1009\n",
      "           5       0.87      0.91      0.89       856\n",
      "           6       0.95      0.94      0.94       961\n",
      "           7       0.92      0.95      0.93      1001\n",
      "           8       0.90      0.90      0.90       977\n",
      "           9       0.90      0.92      0.91       994\n",
      "\n",
      "    accuracy                           0.93     10000\n",
      "   macro avg       0.93      0.93      0.93     10000\n",
      "weighted avg       0.93      0.93      0.93     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(predictions, labels)) # show different metrics for each class in the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
